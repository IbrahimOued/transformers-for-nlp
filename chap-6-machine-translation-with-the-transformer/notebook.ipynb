{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 : Machine Translation with the Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humans master sequence transduction, transferring a representation to another object. We can easily imagine a mental representation of a sequence. If somebody says *The flowers in my garden are beautiful*, we can easily visualize a garden with flowers in it. We see images of the garden, although we might never have seen that garden. We might even imagine chirping birds and the scent of flowers.\n",
    "\n",
    "A machine must learn transduction from scratch with numerical representations. Recurrent or convolutional approaches have produced interesting results but have not reached significant BLEU translation evaluation scores. Translating requires the representation of language A transposed into language B.\n",
    "\n",
    "The transformer model’s self-attention innovation increases the analytic ability of machine intelligence. A sequence in language A is adequately represented before attempting to translate it into language B. Self-attention brings the level of intelligence required by a machine to obtain better BLEU scores.\n",
    "\n",
    "The seminal Attention Is All You Need Transformer obtained the best results for English-German and English-French translations in 2017. Since then, the scores have been improved by other transformers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining machine translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Vaswani et al. (2017)* tackled one of the most difficult NLP problems when designing the Transformer. The human baseline for machine translation seems out of reach for us human-machine intelligence designers. This did not stop *Vaswani et al. (2017)* from publishing the Transformer’s architecture and achieving state-of-the-art BLEU results.\n",
    "\n",
    "Machine translation is the process of reproducing human translation by machine transductions and outputs:\n",
    "\n",
    "![Alt text](machine_translation_process.png)\n",
    "\n",
    "The general idea in Figure 6.1 is for the machine to do the following in a few steps:\n",
    "\n",
    "* Choose a sentence to translate\n",
    "* Learn how words relate to each other with hundreds of millions of parameters\n",
    "* Learn the many ways in which words refer to each other\n",
    "* Use machine transduction to transfer the learned parameters to new sequences\n",
    "* Choose a candidate translation for a word or sequence\n",
    "\n",
    "The process always starts with a sentence to translate from a source language, ***A***. The process ends with an output containing a translated sentence in language ***B***. The intermediate calculations involve transductions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Human transductions and translations**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A human interpreter at the European Parliament, for instance, will not translate a sentence word by word. **Word-by-word translations often make no sense because they lack the proper grammatical structure and cannot produce the right translation because the context of each word is ignored**.\n",
    "\n",
    "**Human transduction takes a sentence in language A and builds a cognitive *representation* of the sentence’s meaning**. An interpreter (oral translations) or a translator (written translations) at the European Parliament will only then **transform that transduction into an interpretation of that sentence in language B**.\n",
    "\n",
    "We will name the translation done by the interpreter or translator in language B a reference sentence.\n",
    "\n",
    "A human translator will not translate sentence A into sentence B several times but only once in real life. However, more than one translator could translate sentence A in real life. For example, you can find several French to English translations of Les Essais by Montaigne. If you take one sentence, A, out of the original French version, you will thus find several versions of sentence B noted as references $1$ to $n$.\n",
    "\n",
    "If you go to the European Parliament one day, you might notice that the interpreters only translate for a limited time of two hours, for example. Then another interpreter takes over. No two interpreters have the same style, just like writers have different styles. Sentence A in the source language might be repeated by the same person several times in a day but be translated into several reference sentence B versions:\n",
    "$$\n",
    "reference ={reference~1, reference~2,\\dots reference~n}\n",
    "$$\n",
    "\n",
    "Machines have to find a way to think the same way as human translators."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Machine transductions and translations**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transduction process of the original Transformer architecture uses the encoder stack, the decoder stack, and all the model’s parameters to represent a *reference sequence*. We will refer to that output sequence as the *reference*.\n",
    "\n",
    "Why not just say \"output prediction\"? The problem is that **there is no single output prediction**. The Transformer, like humans, will produce a result we can refer to, but that can change if we train it differently or use different transformer models!\n",
    "\n",
    "We immediately realize that the human baseline of human transduction, representations of a language sequence, is quite a challenge. However, much progress has been made.\n",
    "\n",
    "An evaluation of machine translation proves that NLP has progressed. To determine that one solution is better than another, each NLP challenger, lab, or organization must refer to the same datasets for the comparison to be valid.\n",
    "\n",
    "Let’s now explore a WMT dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing a WMT dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Vaswani et al. (2017)* present the Transformer’s achievements on the WMT 2014 English-to-German translation task and the WMT 2014 English-to-French translation task. The Transformer achieves a state-of-the-art BLEU score. BLEU will be described in the *Evaluating machine translation with BLEU* section of this chapter.\n",
    "\n",
    "The 2014 WMT contained several European language datasets. One of the datasets contained data taken from version 7 of the Europarl corpus. We will be using the [French-English dataset from the European Parliament Proceedings Parallel Corpus, 1996-2011](https://www.statmt.org/europarl/v7/fr-en.tgz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  193M  100  193M    0     0  2164k      0  0:01:31  0:01:31 --:--:-- 2255k01:37  0:00:36  0:01:01 2049k\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://www.statmt.org/europarl/v7/fr-en.tgz --output \"data/fr-en.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europarl-v7.fr-en.en\n",
      "europarl-v7.fr-en.fr\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf \"data/fr-en.tgz\" --directory \"data/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing the raw data**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n this section, we will preprocess `europarl-v7.fr-en.en` and `europarl-v7.fr-en.fr`.\n",
    "\n",
    "Open `read.py`, which is in this chapter’s GitHub directory. Ensure that the two europarl files are in the same directory as `read.py`.\n",
    "\n",
    "The program begins using standard Python functions and `pickle` to dump the serialized output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_sentences(doc):\n",
    "    return doc.strip().split('\\n')\n",
    "\n",
    "# shortest and longest sentence lengths\n",
    "def sentence_lengths(sentences):\n",
    "    lengths = [len(s.split()) for s in sentences]\n",
    "    return min(lengths), max(lengths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imported sentence lines must be cleaned to avoid training useless and noisy tokens. The lines are normalized, tokenized on white spaces, and converted to lowercase. The punctuation is removed from each token, non-printable characters are removed, and tokens containing numbers are excluded. The cleaned line is stored as a string.\n",
    "\n",
    "The program runs the cleaning function and returns clean appended strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean lines\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "def clean_lines(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for line in lines:\n",
    "        # normalize unicode characters\n",
    "        line = unicodedata.normalize('NFD', line).encode('ascii', 'ignore')\n",
    "        line = line.decode('UTF-8')\n",
    "        # tokenize on white space\n",
    "        line = line.split()\n",
    "        # convert to lower case\n",
    "        line = [word.lower() for word in line]\n",
    "        # remove punctuation from each token\n",
    "        line = [word.translate(table) for word in line]\n",
    "        # remove non-printable chars form each token\n",
    "        line = [re_print.sub('', w) for w in line]\n",
    "        # remove tokens with numbers in them\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "        # store as string\n",
    "        cleaned.append(' '.join(line))\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English data: sentences=2007723, min=0, max=668\n",
      "English.pkl  saved\n"
     ]
    }
   ],
   "source": [
    "# load English data\n",
    "filename = './data/europarl-v7.fr-en.en'\n",
    "doc = load_doc(filename)\n",
    "sentences = to_sentences(doc)\n",
    "minlen, maxlen = sentence_lengths(sentences)\n",
    "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
    "cleanf=clean_lines(sentences)\n",
    "# The dataset is now clean, and pickle dumps it into a serialized file named English.pkl\n",
    "filename = 'English.pkl'\n",
    "outfile = open(filename, 'wb')\n",
    "pickle.dump(cleanf, outfile)\n",
    "outfile.close()\n",
    "print(filename,\" saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now repeat the same process with the French data and dump it into a serialized file named `French.pkl:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French data: sentences=2007723, min=0, max=693\n",
      "French.pkl  saved\n"
     ]
    }
   ],
   "source": [
    "# load French data\n",
    "filename = './data/europarl-v7.fr-en.fr'\n",
    "doc = load_doc(filename)\n",
    "sentences = to_sentences(doc)\n",
    "minlen, maxlen = sentence_lengths(sentences)\n",
    "print('French data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
    "cleanf=clean_lines(sentences)\n",
    "filename = 'French.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(cleanf,outfile)\n",
    "outfile.close()\n",
    "print(filename,\" saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Finalizing the preprocessing of the datasets**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, open `read_clean.py` in the same directory as `read.py`. Our process now defines the function that will load the datasets that were cleaned up in the previous section and then save them once the preprocessing is finalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from collections import Counter\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_sentences(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function that will create a vocabulary counter. It is important to know how many times a word is used in the sequences we will parse. For example, if a word is only used once in a dataset containing two million lines, we will waste our energy using precious GPU resources to learn it. Let’s define the counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a frequency table for all words\n",
    "def to_vocab(lines):\n",
    "    vocab = Counter()\n",
    "    for line in lines:\n",
    "        tokens  = line.split()\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "\n",
    "# remove all words with a frequency below a threshold\n",
    "def trim_vocab(vocab, min_occurrence):\n",
    "        tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
    "        return set(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `min_occurrence=5` and the words below or equal to this threshold have been removed to avoid wasting the training model’s time analyzing them.\n",
    "\n",
    "We now have to deal with **Out-Of-Vocabulary (OOV)**words. **OOV** words **can be misspelled words, abbreviations, or any word that does not fit standard vocabulary representations**. We could use automatic spelling, but it would not solve all of the problems. For this example, we will simply replace **OOV** words with the `unk` (unknown) token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark all OOV with \"unk\" for all lines\n",
    "def update_dataset(lines, vocab):\n",
    "    new_lines = list()\n",
    "    for line in lines:\n",
    "        new_tokens = list()\n",
    "        for token in line.split():\n",
    "            if token in vocab:\n",
    "                new_tokens.append('unk')\n",
    "            else:\n",
    "                new_tokens.append('unk')\n",
    "        new_line = ' '.join(new_tokens)\n",
    "        new_lines.append(new_line)\n",
    "    return new_lines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the functions for the English dataset, save the output, and then display 20 lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary: 105357\n",
      "New English Vocabulary: 41746\n",
      "Saved: english_vocab.pkl\n",
      "line 0 : unk unk unk unk\n",
      "line 1 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 2 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 3 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 4 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 5 : unk unk unk unk unk unk unk unk\n",
      "line 6 : unk unk unk unk unk unk unk unk unk\n",
      "line 7 : unk unk unk unk unk unk unk\n",
      "line 8 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 9 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 10 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 11 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 12 : unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 13 : unk unk unk unk unk unk unk\n",
      "line 14 : unk unk unk unk unk unk unk unk unk\n",
      "line 15 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 16 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 17 : unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 18 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 19 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n"
     ]
    }
   ],
   "source": [
    "# load English dataset\n",
    "filename = 'English.pkl'\n",
    "lines = load_clean_sentences(filename)\n",
    "# calculate vocabulary\n",
    "vocab = to_vocab(lines)\n",
    "print('English Vocabulary: %d' % len(vocab))\n",
    "# reduce vocabulary\n",
    "vocab = trim_vocab(vocab, 5)\n",
    "print('New English Vocabulary: %d' % len(vocab))\n",
    "# mark out of vocabulary words\n",
    "lines = update_dataset(lines, vocab)\n",
    "# save updated dataset\n",
    "filename = 'english_vocab.pkl'\n",
    "save_clean_sentences(lines, filename)\n",
    "# spot check\n",
    "for i in range(20):\n",
    "    print(\"line\",i,\":\",lines[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now run the functions for the French dataset, save the output, and then display $20$ lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Vocabulary: 141642\n",
      "New French Vocabulary: 58800\n",
      "Saved: french_vocab.pkl\n",
      "line 0 : unk unk unk unk\n",
      "line 1 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 2 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 3 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 4 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 5 : unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 6 : unk unk unk unk unk unk unk unk\n",
      "line 7 : unk unk unk unk unk unk unk unk\n",
      "line 8 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 9 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 10 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 11 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 12 : unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 13 : unk unk unk unk unk unk unk unk\n",
      "line 14 : unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 15 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 16 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 17 : unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 18 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n",
      "line 19 : unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk\n"
     ]
    }
   ],
   "source": [
    "# load French dataset\n",
    "filename = 'French.pkl'\n",
    "lines = load_clean_sentences(filename)\n",
    "# calculate vocabulary\n",
    "vocab = to_vocab(lines)\n",
    "print('French Vocabulary: %d' % len(vocab))\n",
    "# reduce vocabulary\n",
    "vocab = trim_vocab(vocab, 5)\n",
    "print('New French Vocabulary: %d' % len(vocab))\n",
    "# mark out of vocabulary words\n",
    "lines = update_dataset(lines, vocab)\n",
    "# save updated dataset\n",
    "filename = 'french_vocab.pkl'\n",
    "save_clean_sentences(lines, filename)\n",
    "# spot check\n",
    "for i in range(20):\n",
    "    print(\"line\",i,\":\",lines[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows how raw data must be processed before training. The datasets are now ready to be plugged into a transformer to be trained.\n",
    "\n",
    "Each line of the French dataset is the sentence to translate. Each line of the English dataset is the reference for a machine translation model. The machine translation model must produce an *English candidate translation* that matches the *reference*.\n",
    "\n",
    "BLEU provides a method to evaluate `candidate` translations produced by machine translation models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluating machine translation with BLEU**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papineni et al. (2002) came up with an efficient way to evaluate a human translation. The human baseline was difficult to define. However, they realized that we could obtain efficient results if we compared human translation with machine translation, word for word.\n",
    "\n",
    "*Papineni et al. (2002)* named their method the **Bilingual Evaluation Understudy Score (BLEU)**.\n",
    "\n",
    "In this section, we will use the **Natural Language Toolkit (NLTK)** to implement [BLEU](http://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu):\n",
    "\n",
    "We will begin with geometric evaluations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Geometric evaluations**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BLEU method **compares the parts of a candidate sentence to a reference sentence or several reference sentences**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It then simulates a comparison between a candidate translation produced by the machine translation model and the actual translation(s) references in the dataset. Remember that a sentence could have been repeated several times and translated by different translators in different ways, making it challenging to find efficient evaluation strategies.\n",
    "\n",
    "The program can evaluate one or more references:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 1.0\n",
      "Example 2 1.0\n"
     ]
    }
   ],
   "source": [
    "#Example 1\n",
    "reference = [['the', 'cat', 'likes', 'milk'], ['cat', 'likes' 'milk']]\n",
    "candidate = ['the', 'cat', 'likes', 'milk']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print('Example 1', score)\n",
    "#Example 2\n",
    "reference = [['the', 'cat', 'likes', 'milk']]\n",
    "candidate = ['the', 'cat', 'likes', 'milk']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print('Example 2', score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A straightforward evaluation P of the candidate C, the reference R, and the number of correct tokens found in C (N) can be represented as a geometric function:\n",
    "\n",
    "$$\n",
    "P(N, C, R) = \\prod^N_{n-1}p_n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This geometric approach is rigid if you are looking for a $3$-gram overlap, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3 1.0547686614863434e-154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibrahim/miniconda3/envs/mlflow/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ibrahim/miniconda3/envs/mlflow/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "#Example 3\n",
    "reference = [['the', 'cat', 'likes', 'milk']]\n",
    "candidate = ['the', 'cat', 'enjoys','milk']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print('Example 3', score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccdc0736e01b132d97e78ee7b194f8632625391a8773c1a4eedba6ee981c132b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
