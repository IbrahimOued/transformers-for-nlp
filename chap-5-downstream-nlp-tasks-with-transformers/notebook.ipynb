{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Downstream NLP tasks with Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers reveal their full potential when we unleash pretrained models and watch them perform downstream **Natural Language Understanding (NLU)** tasks. It takes a lot of time and effort to pretrain and fine-tune a transformer model, but the effort is worthwhile when we see a multi-million parameter transformer model in action on a range of NLU tasks.\n",
    "\n",
    "We will begin this chapter with **the quest of outperforming the human baseline**. The human baseline represents the performance of humans on an NLU task. **Humans learn transduction at an early age and quickly develop inductive thinking**. We humans perceive the world directly with our senses. Machine intelligence relies entirely on our perceptions transcribed into words to make sense of our language.\n",
    "\n",
    "We will then see how to measure the performance of transformers. Measuring **Natural Language Processing (NLP)** tasks remains a straightforward approach involving accuracy scores in various forms based on true and false results. These results are obtained through benchmark tasks and datasets. SuperGLUE, for example, is a wonderful example of how Google DeepMind, Facebook AI, the University of New York, the University of Washington, and others worked together to set high standards to measure NLP performances.\n",
    "\n",
    "Finally, we will explore several downstream tasks, such as the Standard Sentiment TreeBank (SST-2), linguistic acceptability, and Winograd schemas.\n",
    "\n",
    "Transformers are rapidly taking NLP to the next level by outperforming other models on well-designed benchmark tasks. Alternative transformer architectures will continue to emerge and evolve.\n",
    "\n",
    "This chapter covers the following topics:\n",
    "\n",
    "* Machine versus human intelligence for transduction and induction\n",
    "* The NLP transduction and induction process\n",
    "* Measuring transformer performance versus Human Baselines\n",
    "* Measurement methods (Accuracy, F1-score, and MCC)\n",
    "* Benchmark tasks and datasets\n",
    "* SuperGLUE downstream tasks\n",
    "* Linguistic acceptability with CoLA\n",
    "* Sentiment analysis with SST-2\n",
    "* Winograd schemas\n",
    "Let’s start by understanding how humans and machines represent language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transduction and the inductive inheritance of transformers**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The emergence of Automated Machine Learning (AutoML), meaning APIs in automated cloud AI platforms, has deeply changed the job description of every AI specialist. Google Vertex, for example, boasts a reduction of $80\\%$ of the development required to implement ML. This suggests that anybody can implement ML with ready-to-use systems. Does that mean an $80\\%$ reduction of the workforce of developers? I don’t think so. I see an Industry 4.0 AI specialist assemble AI with added value to a project.\n",
    "\n",
    "> Industry 4.0. NLP AI specialists invest less in source code and more in knowledge to become the AI guru of a team.\n",
    "\n",
    "Transformers possess the unique ability to apply their knowledge to tasks they did not learn. A BERT transformer, for example, acquires language through sequence-to-sequence and masked language modeling. The BERT transformer can then be fine-tuned to perform downstream tasks that it did not learn from scratch.\n",
    "\n",
    "In this section, we will do a mind experiment. We will use the graph of a transformer to represent how humans and machines make sense of information using language. Machines make sense of information in a different way from humans but reach very efficient results.\n",
    "\n",
    "Figure 5.1, a mind experiment designed with transformer architecture layers and sublayers, shows the deceptive similarity between humans and machines. Let’s study the learning process of transformer models to understand downstream tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
