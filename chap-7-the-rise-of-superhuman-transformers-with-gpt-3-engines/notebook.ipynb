{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: The Rise of Suprahuman Transformers with GPT-3 Engines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2020, *Brown et al. (2020)* described the training of an OpenAI GPT-3 model containing $175$ billion parameters that learned using huge datasets such as the $400$ billion byte-pair-encoded tokens extracted from Common Crawl data. OpenAI ran the training on a Microsoft Azure supercomputer with $285$ CPUs and $10,000$ GPUs.\n",
    "\n",
    "The machine intelligence of OpenAI’s GPT-3 engines and their supercomputer led *Brown et al. (2020)* to zero-shot experiments. The idea was to **use a trained model for downstream tasks without further training the parameters**. The goal would be for a trained model to **go directly into multi-task production with an API that could even perform tasks it wasn’t trained for**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Suprahuman NLP with GPT-3 transformer models**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-3 is built on the GPT-2 architecture. However, a fully trained GPT-3 transformer is a foundation model. A foundation model can do many tasks it wasn’t trained for. GPT-3 completion applies all NLP tasks and even programming tasks.\n",
    "\n",
    "> GPT-3 is one of the few fully trained transformer models that qualify as a foundation models. GPT-3 will no doubt lead to more powerful OpenAI models. Google will produce foundation models beyond the Google BERT version they trained on their supercomputers. Foundation models represent a new way of thinking about AI.\n",
    "\n",
    "It will not take long for companies to realize they do not need a data scientist or an AI specialist to start an NLP project with an API like the one that OpenAI provides.\n",
    "\n",
    "Why bother with any other tool? An OpenAI API is available with access to one of the most efficient transformer models trained on one of the most powerful supercomputers in the world.\n",
    "\n",
    "Why develop tools, download libraries, or use any other tool if an API exists that only deep pockets and the best research teams in the world can design, such as Google or OpenAI?\n",
    "\n",
    "The answer to these questions is quite simple. It’s easy to start a GPT-3 engine, just as it is to start a Formula 1 or Indy 500 race car. No problem. But then, trying to drive such a car is nearly impossible without months of training! GPT-3 engines are powerful AI race cars. You can get them to run in a few clicks. However, mastering their incredible horsepower requires the knowledge you have acquired from the beginning of this book up to now and what you will discover in the following chapters!\n",
    "\n",
    "We first need to understand the architecture of GPT models to see where developers, AI specialists, and data scientists fit in the era of suprahuman NLP models. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The architecture of OpenAI GPT transformer models**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers went from **training, to fine-tuning, and finally to zero-shot models** in less than three years between the end of 2017 and the first part of 2020. **A zero-shot GPT-3 transformer model requires no fine-tuning**. **The trained model parameters are not updated for downstream multi-tasks, which opens a new era for NLP/NLU tasks**.\n",
    "\n",
    "In this section, we will first learn about the motivation of the OpenAI team that designed GPT models. We will begin by going through the fine-tuning of zero-shot models. Then we will see how to condition a transformer model to generate mind-blowing text completion. Finally, we will explore the architecture of GPT models.\n",
    "\n",
    "We will first go through the creation process of the OpenAI team."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The rise of billion-parameter transformer models**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed at which transformers went from small models trained for NLP tasks to models that require little to no fine-tuning is staggering.\n",
    "\n",
    "*Vaswani et al. (2017)* introduced the Transformer, which surpassed CNNs and RNNs on BLEU tasks. *Radford et al. (2018)* introduced the Generative Pre-Training (GPT) model, which could perform downstream tasks with fine-tuning. *Devlin et al. (2019)* perfected fine-tuning with the BERT model. *Radford et al. (2019)* went further with GPT-2 models.\n",
    "\n",
    "*Brown et al. (2020)* defined a GPT-3 zero-shot approach to transformers that does not require fine-tuning!\n",
    "\n",
    "At the same time, *Wang et al. (2019)* created GLUE to benchmark NLP models. But transformer models evolved so quickly that they surpassed human baselines!\n",
    "\n",
    "*Wang et al. (2019, 2020)* rapidly created SuperGLUE, set the human baselines much higher, and made the NLU/NLP tasks more challenging. Transformers are rapidly progressing, and some have already surpassed Human Baselines on the SuperGLUE leaderboards at the time of writing.\n",
    "\n",
    "How did this happen so quickly?\n",
    "\n",
    "We will look at one aspect, the models’ sizes, to understand how this evolution happened."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The increasing size of transformer models**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 2017 to 2020 alone, the number of parameters increased from 65M parameters in the original Transformer model to 175B parameters in the GPT-3 model, as shown in Table 7.1:\n",
    "\n",
    "| **Transformer Model** | **Paper**               | **Parameters** |\n",
    "| --------------------- | ----------------------- | -------------- |\n",
    "| Transformer Base      | _Vaswani et al. (2017)_ | 65M            |\n",
    "| Transformer Big       | _Vaswani et al. (2017)_ | 213M           |\n",
    "| BERT-Base             | _Devlin et al. (2019)_  | 110M           |\n",
    "| BERT-Large            | _Devlin et al. (2019)_  | 340M           |\n",
    "| GPT-2                 | _Radford et al. (2019)_ | 117M           |\n",
    "| GPT-2                 | _Radford et al. (2019)_ | 345M           |\n",
    "| GPT-2                 | _Radford et al. (2019)_ | 1.5B           |\n",
    "| GPT-3                 | _Radford et al. (2019)_ | 175B           |\n",
    "\n",
    "Table 7.1 only contains the main models designed during that short time. The dates of the publications come after the date the models were actually designed. Also, the authors updated the papers. For example, once the original Transformer set the market in motion, transformers emerged from Google Brain and Research, OpenAI, and Facebook AI, which all produced new models in parallel.\n",
    "\n",
    "Furthermore, some GPT-2 models are larger than the smaller GPT-3 models. For example, the GPT-3 Small model contains 125M parameters, which is smaller than the 345M parameter GPT-2 model.\n",
    "\n",
    "The size of the architecture evolved at the same time:\n",
    "\n",
    "The number of layers of a model went from 6 layers in the original Transformer to 96 layers in the GPT-3 model\n",
    "* The number of heads of a layer went from 8 in the original Transformer model to 96 in the GPT-3 model\n",
    "* The context size went from 512 tokens in the original Transformer model to 12,288 in the GPT-3 model\n",
    "* The architecture’s size explains why GPT-3 175B, with its 96 layers, produces more impressive results than GPT-2 1,542M, with only 40 layers. The parameters of both models are comparable, but the number of layers has doubled.\n",
    "\n",
    "Let’s focus on the context size to understand another aspect of the rapid evolution of transformers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context size and maximum path length**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The cornerstone of transformer models resides in the attention sub-layers**. In turn, **the key property of attention sub-layers is the method used to process context size**.\n",
    "\n",
    "The **context size is one of the main ways humans and machines can learn languages**. The **larger the context size, the more we can understand a sequence** presented to us.\\\n",
    "However, **the drawback of context size is the distance it takes to understand what a word refers to**. The **path taken to analyze long-term dependencies requires changing from recurrent to attention layers**.\n",
    "\n",
    "The following sentence requires a long path to find what the pronoun <font color=pink>\"it\"</font> refers to:\n",
    "\n",
    "<font color=yellow>\"Our house was too small to fit a big couch, a large table, and other furniture we would have liked in such a tiny space. We thought about staying for some time, but finally, we decided to sell it.\"</font>\n",
    "\n",
    "The meaning of <font color=pink>\"it\"</font> can only be explained if we take a long path back to the word <font color=pink>\"house\"</font> at the beginning of the sentence. That’s quite a path for a machine!\n",
    "\n",
    "The order of function that defines the maximum path length can be summed up as shown in Table 7.2 in Big O notation:\n",
    "\n",
    "| **Layer Type** | **Maximum Path Length** | **Context Size** |\n",
    "| -------------- | ----------------------- | ---------------- |\n",
    "| Self-Attention | $\\Omicron(1)$           | $1$              |\n",
    "| Reccurent      | $\\Omicron(n)$           | $100$            |\n",
    "\n",
    "*Vaswani et al. (2017)* **optimized the design of context analysis in the original Transformer model**. **Attention brings the operations down to a one-to-one token operation**. The fact that all of the layers are identical makes it much easier to scale up the size of transformer models. A GPT-3 model with a size 100 context window has the same maximum length path as a size 10 context window.\n",
    "\n",
    "For example, a**recurrent layer in an RNN has to store the total length of the context step by step**. The **maximum path length is the context size**. The maximum length size for an RNN that would process the context size of a GPT-3 model would be $\\Omicron(n)$ times longer. Furthermore, **an RNN cannot split the context into $96$ heads running on a parallelized machine architecture**, distributing the operations over $96$ GPUs, for example.\n",
    "The flexible and optimized architecture of transformers has led to an impact on several other factors:\n",
    "* *Vaswani* et al. (2017) trained a state-of-the-art transformer model with 36M sentences. *Brown* et al. (2020) trained a GPT-3 model with 400 billion byte-pair-encoded tokens extracted from Common Crawl data.\n",
    "* Training large transformer models requires machine power that is only available to a small number of teams in the world. It took a total of $2.14*10^{23}$ FLOPS for *Brown* et al. (2020) to train GPT-3 175B.\n",
    "* Designing the architecture of transformers requires highly qualified teams that can only be funded by a small number of organizations in the world.\n",
    "The size and architecture will continue to evolve and probably increase to trillion-parameter models in the near future. Supercomputers will continue to provide the necessary resources to train transformers.\n",
    "\n",
    "We will now see how zero-shot models were achieved.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **From fine-tuning to zero-shot models**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the start, OpenAI’s research teams, led by *Radford* et al. (2018), wanted to **take transformers from trained models to GPT models**. The goal was to **train transformers on unlabeled data. Letting attention layers learn a language from unsupervised data was a smart move**. Instead of teaching transformers to do specific NLP tasks, OpenAI decided to train transformers to learn a language.\n",
    "\n",
    "OpenAI wanted to **create a task-agnostic model**. So they began to train transformer models on raw data instead of relying on labeled data by specialists. Labeling data is time-consuming and considerably slows down the transformer’s training process.\n",
    "\n",
    "The first step was to start with unsupervised training in a transformer model. Then, they would only fine-tune the model’s supervised learning.\n",
    "\n",
    "OpenAI opted for **a decoder-only transformer** described in the Stacking decoder layers section. **The metrics of the results were convincing and quickly reached the level of the best NLP models of fellow NLP research labs**.\n",
    "\n",
    "The promising results of the first version of GPT transformer models soon led *Radford* et al. (2019) to come up with zero-shot transfer models. The core of their philosophy was to continue training GPT models to learn from raw text. They then took their research a step further, focusing on language modeling through examples of unsupervised distributions:\n",
    "\n",
    "$$\n",
    "Examples=(x_1, x_2, x_3, \\dots ,x_n)\n",
    "$$\n",
    "\n",
    "The examples are composed of sequences of symbols:\n",
    "$$\n",
    "Sequences=(s1, s2, s3, \\dots,sn)\n",
    "$$\n",
    "\n",
    "This led to a metamodel that can be expressed as a probability distribution for any type of input:\n",
    "$$\n",
    "p(output/input)\n",
    "$$\n",
    "\n",
    "The goal was to generalize this concept to any type of downstream task once the trained GPT model understands a language through intensive training.\n",
    "\n",
    "The GPT models rapidly evolved from $117\\textbf{M}$ parameters to $345\\textbf{M}$ parameters, to other sizes, and then to $1,542\\textbf{M}$ parameters. $1,000,000,000+$ parameter transformers were born. **The amount of fine-tuning was sharply reduced**. The results reached state-of-the-art metrics again.\n",
    "\n",
    "This encouraged OpenAI to go further, much further. *Brown* et al. (2020) went on the assumption that **conditional probability transformer models could be trained in-depth and were able to produce excellent results with little to no fine-tuning for downstream tasks**:\n",
    "$$\n",
    "p(output/multi\\text{-}tasks)\n",
    "$$\n",
    "\n",
    "OpenAI was reaching its goal of training a model and then running downstream tasks directly without further fine-tuning. This phenomenal progress can be described in four phases:\n",
    "* **Fine-Tuning (FT)** is meant to be performed in the sense we have been exploring in previous chapters. A transformer model is trained and then fine-tuned on downstream tasks. *Radford* et al. (2018) designed many fine-tuning tasks. The OpenAI team then reduced the number of tasks progressively to $0$ in the following steps.\n",
    "* **Few-Shot (FS)** represents a huge step forward. The GPT is trained. When the model needs to make inferences, it is presented with demonstrations of the task to perform as conditioning. Conditioning replaces weight updating, which the GPT team excluded from the process. We will be applying conditioning to our model through the context we provide to obtain text completion in the notebooks we will go through in this chapter.\n",
    "* **One-Shot (1S)** takes the process further. The trained GPT model is presented with only one demonstration of the downstream task to perform. No weight updating is permitted either.\n",
    "* **Zero-Shot (ZS)** is the ultimate goal. The trained GPT model is presented with no demonstration of the downstream task to perform.\n",
    "\n",
    "Each of these approaches has various levels of efficiency. The OpenAI GPT team has worked hard to produce these state-of-the-art transformer models.\n",
    "\n",
    "We can now explain the motivations that led to the architecture of the GPT models:\n",
    "\n",
    "* Teaching transformer models how to learn a language through extensive training.\n",
    "* Focusing on language modeling through context conditioning.\n",
    "* The transformer takes the context and generates text completion in a novel way. Instead of consuming resources on learning downstream tasks, it works on understanding the input and making inferences no matter what the task is.\n",
    "* Finding efficient ways to train models by masking portions of the input sequences forces the transformer to think with machine intelligence. Thus, machine intelligence, though not human, is efficient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stacking decoder layers**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now understand that the OpenAI team focused on language modeling. Therefore, **it makes sense to keep the masked attention sublayer**. Hence, **the choice to retain the decoder stacks and exclude the encoder stacks**. *Brown* et al. (2020) **dramatically increased the size of the decoder-only transformer models to get excellent results**.\n",
    "\n",
    "GPT models have the same structure as the decoder stacks of the original Transformer designed by *Vaswani* et al. (2017). We described the decoder stacks in Chapter 2, Getting Started with the Architecture of the Transformer Model. If necessary, take a few minutes to go back through the architecture of the original Transformer.\n",
    "\n",
    "The GPT model has a decoder-only architecture, as shown in Figure 7.1:\n",
    "\n",
    "![Alt text](gpt_decoder.png)\n",
    "\n",
    "We can recognize the text and position embedding sub-layer, the masked multi-head self-attention layer, the normalization sub-layers, the feedforward sub-layer, and the outputs. In addition, there is a version of GPT-2 with both text prediction and task classification.\n",
    "\n",
    "The OpenAI team customized and tweaked the decoder model by model. Radford et al. (2019) presented no fewer than four GPT models, and Brown et al. (2020) described no fewer than eight models.\n",
    "\n",
    "The GPT-3 175B model has reached a unique size that requires computer resources that few teams in the world can access:\n",
    "\n",
    "$$\n",
    "n_{params} = 175.0\\textbb{B}, n_{layers} = 96, d_{model} = 12288, n_{heads} = 96\n",
    "$$\n",
    "\n",
    "Let’s look into the growing number of GPT-3 engines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GPT-3 engines**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GPT-3 model can be trained to accomplish specific tasks of different sizes. The list of engines available at this time is documented by OpenAI: https://beta.openai.com/docs/engines\n",
    "\n",
    "The base series of engines have different functions – for example:\n",
    "\n",
    "* The **Davinci** engine can analyze complex intent\n",
    "* The **Curie** engine is fast and has good summarization\n",
    "* The **Babbage** engine is good at semantic search\n",
    "* The **Ada** engine is good at parsing text\n",
    "\n",
    "OpenAI is producing more engines to put on the market:\n",
    "\n",
    "* The **Instruct series** provides instructions based on a description. An example is available in the More GPT-3 examples section of this chapter.\n",
    "* The **Codex series** can translate language to code. We will explore this series in *Chapter 16, The Emergence of Transformer-Driven Copilots*.\n",
    "* The **Content filter series** filters unsafe or sensitive text. We will explore this series in *Chapter 16, The Emergence of Transformer-Driven Copilots*.\n",
    "\n",
    "We have explored the process that led us from fine-tuning to zero-shot GPT-3 models. We have seen that GPT-3 can produce a wide range of engines. It is now time to see how the source code of GPT models is built. Although the GPT-3 transformer model source code is not publicly available at this time, GPT-2 models are sufficiently powerful to understand the inner workings of GPT models.\n",
    "\n",
    "We are ready to interact with a GPT-2 model and train it.\n",
    "\n",
    "We will first use a trained GPT-2 345M model for text completion with 24 decoder layers with self-attention sublayers of 16 heads.\n",
    "\n",
    "We will then train a GPT-2 117M model for customized text completion with 12 decoder layers with self-attention layers of 12 heads.\n",
    "\n",
    "Let’s start by interacting with a pretrained 345M parameter GPT-2 model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generic text completion with GPT-2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
