{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 : Pretraining a RoBERTa model from scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a RoBERTa model from scratch. The model will use the **bricks of the transformer construction kit we need for BERT models**. Also, no pretrained tokenizers or models will be used. The RoBERTa model will be built following the $15$-step process described in this chapter.\n",
    "\n",
    "This chapter will focus on building a pretrained transformer model from scratch using a Jupyter notebook based on Hugging Face’s seamless modules. The model is named KantaiBERT.\n",
    "\n",
    "**KantaiBERT first loads a compilation of Immanuel Kant’s books created for this chapter**. You will see how the data was obtained. You will also see how to create your own datasets for this notebook.\n",
    "\n",
    "**KantaiBERT trains its own tokenizer from scratch**. It will build its merge and vocabulary files, which will be used during the pretraining process.\n",
    "\n",
    "**KantaiBERT then processes the dataset, initializes a trainer, and trains the model**.\n",
    "\n",
    "Finally, **KantaiBERT uses the trained model to perform an experimental downstream language modeling task and fills a mask using Immanuel Kant’s logic**.\n",
    "\n",
    "By the end of the chapter, you will know how to build a transformer model from scratch. You will have enough knowledge of transformers to face the Industry 4.0 challenge of using powerful pretrained transformers such as GPT-3 engines that require more than development skills to implement them. This chapter prepares you for *Chapter 7, The Rise of Suprahuman Transformers with GPT-3 Engines*.\n",
    "\n",
    "This chapter covers the following topics:\n",
    "\n",
    "* *RoBERTa*- and *DistilBERT*-like models\n",
    "* How to train a tokenizer from scratch\n",
    "* Byte-level byte-pair encoding\n",
    "* Saving the trained tokenizer to files\n",
    "* Recreating the tokenizer for the pretraining process\n",
    "* Initializing a *RoBERTa* model from scratch\n",
    "* Exploring the configuration of the model\n",
    "* Exploring the $80$ million parameters of the model\n",
    "* Building the dataset for the trainer\n",
    "* Initializing the trainer\n",
    "* Pretraining the model\n",
    "* Saving the model\n",
    "* Applying the model to the downstream tasks of **Masked Language Modeling (MLM)**\n",
    "\n",
    "Our first step will be to describe the transformer model that we are going to build."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training a tokenizer and pretraining a transformer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a transformer model named KantaiBERT using the building blocks provided by Hugging Face for BERT-like models. We covered the theory of the building blocks of the model we will be using in *Chapter 3*.\n",
    "\n",
    "We will describe KantaiBERT, building on the knowledge we acquired in previous chapters.\n",
    "\n",
    "KantaiBERT is a **Robustly Optimized BERT Pretraining Approach (RoBERTa)**-like model based on the architecture of BERT.\n",
    "\n",
    "The initial BERT models brought innovative features to the initial transformer models, as we saw in Chapter 3. RoBERTa increases the performance of transformers for downstream tasks by improving the mechanics of the pretraining process.\n",
    "\n",
    "For example, it does not use **WordPiece** tokenization but goes down to byte-level **Byte-Pair Encoding (BPE)**. This method paved the way for a wide variety of BERT and BERT-like models.\n",
    "\n",
    "In this chapter, KantaiBERT, like BERT, will be trained using **Masked Language Modeling (MLM)**. **MLM is a language modeling technique that masks a word in a sequence**. The transformer model must train to predict the masked word.\n",
    "\n",
    "KantaiBERT will be trained as a small model with $6$ layers, $12$ heads, and $84,095,008$ parameters. It might seem that $84$ million parameters is a lot. However, the parameters are spread over $12$ heads, which makes it a relatively small model. A small model will make the pretraining experience smooth so that each step can be viewed in real time without waiting for hours to see a result.\n",
    "\n",
    "**KantaiBERT is a DistilBERT-like model because it has the same architecture of $6$ layers and $12$ heads**. **DistilBERT is a distilled version of BERT**. DistilBERT, as the name suggests, contains fewer parameters than a RoBERTa model. As such, it runs much faster, but the results are slightly less accurate than with a RoBERTa model.\n",
    "\n",
    "We know that large models achieve excellent performance. But what if you want to run a model on a smartphone? Distillation using fewer parameters or other such methods in the future is a clever way of taking the best of pretraining and making it efficient for the needs of many downstream tasks.\n",
    "\n",
    "KantaiBERT will implement a byte-level byte-pair encoding tokenizer like the one used by GPT-2. The special tokens will be the ones used by RoBERTa. BERT models most often use a WordPiece tokenizer.\n",
    "\n",
    "There are no token type IDs to indicate which part of a segment a token is a part of. The segments will be separated with the separation token `</s>`.\n",
    "\n",
    "KantaiBERT will use a custom dataset, train a tokenizer, train the transformer model, save it, and run it with an MLM example.\n",
    "\n",
    "Let’s get going and build a transformer from scratch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building KantaiBERT from scratch**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build KantaiBERT in 15 steps from scratch and run it on an MLM example.\n",
    "\n",
    "Open the `KantaiBERT_Repro.ipynb` file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Loading the dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready-to-use datasets provide an objective way to train and compare transformers. In Chapter 5, Downstream NLP Tasks with Transformers, we will explore several datasets. However, this chapter aims to understand the training process of a transformer with notebook cells that can be run in real time without waiting for hours to obtain a result.\n",
    "\n",
    "I chose to use the works of Immanuel Kant (1724-1804), the German philosopher who was the epitome of the Age of Enlightenment. The idea is to introduce human-like logic and pretrained reasoning for downstream reasoning tasks.\n",
    "\n",
    "[Project Gutenberg](https://www.gutenberg.org), offers a wide range of free eBooks that can be downloaded in text format. You can use other books if you want to create customized datasets of your own based on books.\n",
    "\n",
    "I compiled the following three books by Immanuel Kant into a text file named kant.txt:\n",
    "\n",
    "* The Critique of Pure Reason\n",
    "* The Critique of Practical Reason\n",
    "* Fundamental Principles of the Metaphysic of Morals\n",
    "\n",
    "`kant.txt` provides a small training dataset to train the transformer model of this chapter. The result obtained remains experimental. For a real-life project, I would add the complete works of Immanuel Kant, Rene Descartes, Pascal, and Leibnitz, for example.\n",
    "\n",
    "The text file contains the raw text of the books:\n",
    "> ...For it is in reality vain to profess _indifference_ in regard to such inquiries, the object of which cannot be indifferent to humanity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Installing Hugging Face transformers**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to install Hugging Face transformers and tokenizers, but we will not need TensorFlow in this instance of the Google Colab VM so we can remove it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Training a tokenizer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, **the program does not use a pretrained tokenizer**. For example, **a pretrained GPT-2 tokenizer could be used**. However, **the training process in this chapter includes training a tokenizer from scratch**.\n",
    "\n",
    "**Hugging Face**’s `ByteLevelBPETokenizer()` will be trained using `kant.txt`. A BPE tokenizer **will break a string or word down into substrings or subwords**. There are two main advantages to this, among many others:\n",
    "\n",
    "- The tokenizer can break words into minimal components. Then it will merge these small components into statistically interesting ones. For example, “smaller\" and smallest\" can become “small,” “er,” and “est.” The tokenizer can go further. We could get “sm\" and “all,” for example. In any case, the words are broken down into subword tokens and smaller units of subword parts such as “sm\" and “all\" instead of simply “small.”\n",
    "- The chunks of strings classified as unknown, `unk_token`, using `WordPiece` level encoding, will practically disappear.\n",
    "\n",
    "In this model, we will be training the tokenizer with the following parameters:\n",
    "\n",
    "- `files=paths` is the path to the dataset\n",
    "- `vocab_size=52_000` is the size of our tokenizer’s model length\n",
    "- `min_frequency=2` is the minimum frequency threshold\n",
    "- `special_tokens=[]` is a list of special tokens\n",
    "\n",
    "In this case, the list of special tokens is:\n",
    "\n",
    "- `<s>`: a start token\n",
    "- `<pad>`: a padding token\n",
    "- `</s>`: an end token\n",
    "- `<unk>`: an unknown token\n",
    "- `<mask>`: the mask token for language modeling\n",
    "\n",
    "The tokenizer will be trained to generate merged substring tokens and analyze their frequency.\n",
    "\n",
    "Let’s take these two words in the middle of a sentence:\n",
    "\n",
    "The first step will be to tokenize the string:\n",
    "\n",
    "```py\n",
    "'Ġthe', 'Ġtoken',   'izer',\n",
    "```\n",
    "\n",
    "The string is now tokenized into tokens with Ġ (whitespace) information.\n",
    "\n",
    "The next step is to replace them with their indices:\n",
    "\n",
    "| ‘Ġthe’ | ‘Ġtoken’ | ‘izer’ |\n",
    "| ------ | -------- | ------ |\n",
    "| 150    | 5430     | 4712   |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Saving the files to disk**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he tokenizer will generate two files when trained:\n",
    "\n",
    "* `merges.txt`, which contains the merged tokenized substrings\n",
    "* `vocab.json`, which contains the indices of the tokenized substrings\n",
    "\n",
    "The program first creates the `KantaiBERT` directory and then saves the two files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccdc0736e01b132d97e78ee7b194f8632625391a8773c1a4eedba6ee981c132b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
